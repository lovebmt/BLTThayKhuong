// Slides Data - AdaGrad Algorithm Presentation
// Presentation Flow: T1 (Intro) â†’ T2 (Core Mechanism) â†’ T3 (Algorithm & Examples) â†’ T4 (Experiments & Evolution) â†’ T1 (Summary & Close)
const SLIDES_DATA = {
  "presentation": {
    "title": "AdaGrad: Adaptive Gradient Algorithm",
    "totalSlides": 25
  },
  "slides": [
    // ========== T1: Má»Ÿ Ä‘áº§u (3 slides) ==========
    {
      "id": 1,
      "type": "title",
      "title": "AdaGrad Algorithm",
      "subtitle": "Understanding Adaptive Learning Through First Principles",
      "badges": [
        { "text": "Adaptive Optimization", "color": "blue" },
        { "text": "Sparse Features", "color": "green" },
        { "text": "Step-by-Step Learning", "color": "purple" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "NhÃ³m BÃ¡o CÃ¡o",
          "cards": [
            {
              "icon": "ğŸ‘¥",
              "iconColor": "purple",
              "title": "ThÃ nh viÃªn",
              "content": "LÃª PhÆ°á»›c ThÃ nh<br>LÃª PhÆ°á»›c ThÃ nh <br>Nguyá»…n ThÃ nh Äáº¡t<br>LÃª Äá»©c PhÆ°Æ¡ng"
            }
          ]
        },
        {
          "title": "Má»¥c tiÃªu há»c táº­p",
          "cards": [
            {
              "icon": "ğŸ’¡",
              "iconColor": "green",
              "title": "Hiá»ƒu sÃ¢u cÆ¡ cháº¿",
              "content": "Táº¡i sao cáº§n adaptive LR?<br>Squared gradients lÃ  gÃ¬?<br>Preconditioning nghÄ©a lÃ  gÃ¬?"
            },
            {
              "icon": "ğŸ”",
              "iconColor": "orange",
              "title": "TÃ­nh toÃ¡n tá»«ng bÆ°á»›c",
              "content": "Manual calculations<br>Geometric interpretation<br>Khi nÃ o dÃ¹ng, khi nÃ o trÃ¡nh"
            }
          ]
        }
      ]
    },
    {
      "id": 2,
      "type": "algorithm",
      "title": "CÃ¢u há»i khá»Ÿi Ä‘áº§u: Táº¡i sao cáº§n Adaptive Learning Rate?",
      "subtitle": "Hiá»ƒu váº¥n Ä‘á» tá»« sparse features",
      "badges": [
        { "text": "Problem Definition", "color": "red" },
        { "text": "Real-world Example", "color": "blue" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Bá»‘i cáº£nh & Váº¥n Ä‘á»",
          "cards": [
            {
              "icon": "ğŸ“š",
              "iconColor": "blue",
              "title": "Text Classification vá»›i Bag-of-Words",
              "content": "Dictionary: 10,000 tá»«<br>Má»—i document chá»‰ cÃ³ ~50 tá»« active<br>99.5% features = 0 (sparse!)"
            },
            {
              "icon": "âŒ",
              "iconColor": "red",
              "title": "Váº¥n Ä‘á» vá»›i Global Learning Rate Decay",
              "content": "Î·<sub>t</sub> = Î·<sub>0</sub>/âˆšt giáº£m theo thá»i gian<br><br>Tá»« hiáº¿m xuáº¥t hiá»‡n láº§n Ä‘áº§u á»Ÿ step t=1000:<br>â†’ Î·<sub>1000</sub> = 0.1/âˆš1000 â‰ˆ 0.003<br>â†’ Update siÃªu nhá» dÃ¹ gradient cÃ³ thá»ƒ lá»›n!"
            }
          ]
        },
        {
          "title": "CÃ¢u há»i quan trá»ng",
          "cards": [
            {
              "icon": "ğŸ¤”",
              "iconColor": "orange",
              "title": "LÃ m sao Ä‘á»ƒ tá»« hiáº¿m váº«n há»c Ä‘Æ°á»£c hiá»‡u quáº£?",
              "content": "<strong style='color:#ff5722;'>Cáº§n cÆ¡ cháº¿ Ä‘iá»u chá»‰nh learning rate <em>riÃªng cho tá»«ng parameter</em> dá»±a trÃªn lá»‹ch sá»­ gradient cá»§a nÃ³!</strong><br><br>Global LR decay khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c parameter nÃ o cáº§n há»c nhiá»u/Ã­t"
            }
          ]
        }
      ]
    },
    {
      "id": 3,
      "type": "algorithm",
      "title": "Naive Solution: Äáº¿m sá»‘ láº§n xuáº¥t hiá»‡n",
      "subtitle": "Ã tÆ°á»Ÿng Ä‘Æ¡n giáº£n nhÆ°ng sai láº§m",
      "badges": [
        { "text": "Simple Idea", "color": "blue" },
        { "text": "Why It Fails", "color": "red" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Ã tÆ°á»Ÿng Ä‘Æ¡n giáº£n",
          "cards": [
            {
              "icon": "ğŸ’­",
              "iconColor": "blue",
              "title": "Äáº¿m sá»‘ láº§n feature xuáº¥t hiá»‡n",
              "content": "s(i,t) = sá»‘ láº§n feature i xuáº¥t hiá»‡n Ä‘áº¿n step t<br>Î·<sub>i,t</sub> = Î·<sub>0</sub>/âˆš(s(i,t) + c)<br><br>Feature hiáº¿m: s nhá» â†’ Î· lá»›n âœ“<br>Feature thÆ°á»ng xuyÃªn: s lá»›n â†’ Î· nhá» âœ“"
            },
            {
              "icon": "ğŸ’¡",
              "iconColor": "green",
              "title": "algorithm quan trá»ng",
              "content": "<strong style='color:#4CAF50;'>Cáº§n cÆ¡ cháº¿ tá»± Ä‘á»™ng cÃ¢n nháº¯c magnitude!</strong><br><br>â†’ KhÃ´ng Ä‘áº¿m, mÃ  <em>tÃ­ch lÅ©y squared gradients</em><br>â†’ Gradient lá»›n Ä‘Ã³ng gÃ³p nhiá»u hÆ¡n (Â²)"
            }
          ]
        },
        {
          "title": "3 váº¥n Ä‘á» nghiÃªm trá»ng",
          "cards": [
            {
              "icon": "âŒ",
              "iconColor": "red",
              "title": "Táº¡i sao counting khÃ´ng Ä‘á»§?",
              "content": "<strong>1. Binary decision:</strong> Gradient 0.001 = gradient 10?<br><br><strong>2. Bá» qua magnitude:</strong> KhÃ´ng phÃ¢n biá»‡t gradient lá»›n/nhá»<br><br><strong>3. Threshold arbitrary:</strong> Bao giá» coi lÃ  'xuáº¥t hiá»‡n'?<br><br>Cáº§n phÆ°Æ¡ng phÃ¡p continuous, automatic!"
            }
          ]
        }
      ]
    },
    {
      "id": 4,
      "type": "algorithm",
      "title": "Äá»™t phÃ¡: Accumulated Squared Gradients",
      "subtitle": "Core innovation cá»§a AdaGrad",
      "badges": [
        { "text": "Key algorithm", "color": "green" },
        { "text": "Automatic Weighting", "color": "blue" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "CÆ¡ cháº¿ tÃ­ch lÅ©y má»›i",
          "cards": [
            {
              "icon": "ğŸ’¡",
              "iconColor": "blue",
              "title": "1. Thay Ä‘á»•i cÃ¡ch tÃ­ch lÅ©y",
              "content": "<strong>Thay vÃ¬:</strong> s(i,t) = count xuáº¥t hiá»‡n<br><br><strong>DÃ¹ng:</strong> s(i,t+1) = s(i,t) + (âˆ‚<sub>i</sub>f)<sup>2</sup><br><br><span style='color:#4CAF50;'>â†’ Gradient cÃ ng lá»›n, Ä‘Ã³ng gÃ³p cÃ ng nhiá»u!</span>"
            },
            {
              "icon": "ğŸ¯",
              "iconColor": "green",
              "title": "2. Per-coordinate learning rate",
              "content": "Î·<sub>i,t</sub> = Î· / âˆš(s(i,t) + Îµ)<br><br>Má»—i parameter cÃ³ <em>learning rate riÃªng</em><br>tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh theo lá»‹ch sá»­ gradient"
            }
          ]
        },
        {
          "title": "Tá»± Ä‘á»™ng cÃ¢n báº±ng",
          "cards": [
            {
              "icon": "âš–ï¸",
              "iconColor": "purple",
              "title": "3. CÆ¡ cháº¿ tá»± Ä‘iá»u chá»‰nh",
              "content": "<strong>Gradient lá»›n/thÆ°á»ng xuyÃªn:</strong><br>s tÄƒng nhanh â†’ Î· hiá»‡u dá»¥ng nhá» (phanh!)<br><br><strong>Gradient nhá»/hiáº¿m:</strong><br>s tÄƒng cháº­m â†’ Î· hiá»‡u dá»¥ng lá»›n (tÄƒng tá»‘c!)<br><br><span style='color:#2196F3;'>Tá»± Ä‘á»™ng cÃ¢n nháº¯c magnitude!</span>"
            }
          ]
        }
      ]
    },
    {
      "id": 5,
      "type": "algorithm",
      "title": "Deep Dive: Preconditioning - Táº¡i sao 'lÃ m trÃ²n' landscape?",
      "subtitle": "Hiá»ƒu geometric meaning cá»§a AdaGrad",
      "badges": [
        { "text": "Advanced Concept", "color": "purple" },
        { "text": "Geometric Intuition", "color": "blue" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Váº¥n Ä‘á» & Má»¥c tiÃªu",
          "cards": [
            {
              "icon": "ğŸ”ï¸",
              "iconColor": "red",
              "title": "Landscape bá»‹ kÃ©o dÃ i (elongated)",
              "content": "Quadratic f(x) = Â½x<sup>T</sup>Qx<br><br><strong>Condition number:</strong> Îº(Q) = Î»<sub>max</sub>/Î»<sub>min</sub><br><br>Îº lá»›n â†’ contour thÃ nh ellipse dÃ i<br>â†’ GD zigzag, há»™i tá»¥ cháº­m"
            },
            {
              "icon": "ğŸ¯",
              "iconColor": "blue",
              "title": "Má»¥c tiÃªu: Biáº¿n ellipse thÃ nh sphere",
              "content": "<strong>Preconditioning:</strong> Äá»•i biáº¿n y = P<sup>1/2</sup>x<br>Hessian má»›i: QÌƒ = P<sup>-1/2</sup>QP<sup>-1/2</sup><br><br>LÃ½ tÆ°á»Ÿng: P=Q â†’ QÌƒ=I â†’ Îº=1 (hÃ¬nh trÃ²n!)<br><br><span style='color:#f44336;'>NhÆ°ng:</span> TÃ­nh P=Q cáº§n O(dÂ²) memory - khÃ´ng kháº£ thi!"
            }
          ]
        },
        {
          "title": "AdaGrad's Solution",
          "cards": [
            {
              "icon": "ğŸ’¡",
              "iconColor": "green",
              "title": "Diagonal approximation - Máº¹o thÃ´ng minh!",
              "content": "<strong>Chá»‰ dÃ¹ng diagonal:</strong> D<sub>t</sub> = diag(s<sub>t</sub>)<br><br>x<sub>t+1</sub> = x<sub>t</sub> - Î· D<sub>t</sub><sup>-1/2</sup> g<sub>t</sub><br><br><span style='color:#4CAF50;'>â†’ O(d) memory, cheap updates</span><br>â†’ Hoáº¡t Ä‘á»™ng tá»‘t khi Q gáº§n diagonal (axis-aligned)<br>â†’ 'LÃ m trÃ²n' landscape tá»± Ä‘á»™ng!<br>â†’ Má»—i direction cÃ³ scaling riÃªng"
            }
          ]
        }
      ]
    },
    {
      "id": 6,
      "type": "algorithm",
      "title": "Gradient as Hessian Proxy - Máº¹o thÃ´ng minh!",
      "subtitle": "Táº¡i sao khÃ´ng cáº§n tÃ­nh Hessian?",
      "badges": [
        { "text": "First-Order Only", "color": "blue" },
        { "text": "Cheap Trick", "color": "green" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Váº¥n Ä‘á» vá»›i Hessian",
          "cards": [
            {
              "icon": "ğŸš«",
              "iconColor": "red",
              "title": "KhÃ´ng kháº£ thi trong Deep Learning",
              "content": "d = 10<sup>6</sup> parameters<br>Hessian: dÃ—d matrix = 10<sup>12</sup> entries!<br><br>Memory: O(dÂ²) - khÃ´ng thá»ƒ lÆ°u<br>Inversion: O(dÂ³) - quÃ¡ cháº­m<br><br>Cáº§n approximation thÃ´ng minh!"
            },
            {
              "icon": "ğŸ¤”",
              "iconColor": "blue",
              "title": "Quan sÃ¡t quan trá»ng",
              "content": "Gáº§n optimal x*: g(x) â‰ˆ Q(x - x*)<br><br><strong>Curvature cao (Q<sub>ii</sub> lá»›n):</strong><br>â†’ Gradient thay Ä‘á»•i nhiá»u<br>â†’ |g<sub>i</sub>| lá»›n hoáº·c variance cao<br><br><strong>Curvature tháº¥p (Q<sub>ii</sub> nhá»):</strong><br>â†’ Gradient thay Ä‘á»•i Ã­t â†’ |g<sub>i</sub>| nhá»"
            }
          ]
        },
        {
          "title": "AdaGrad's Approximation",
          "cards": [
            {
              "icon": "âœ¨",
              "iconColor": "green",
              "title": "Use gradients as proxy!",
              "content": "<strong>Thay vÃ¬:</strong> TÃ­nh diagonal cá»§a Hessian Q<br><br><strong>DÃ¹ng:</strong> Accumulated squared gradients<br>s<sub>i</sub> = Î£g<sub>i</sub>Â² â‰ˆ rough proxy cho Q<sub>ii</sub><br><br><span style='color:#4CAF50;'>â†’ O(d) storage, already computed!</span><br>â†’ Gets curvature info 'for free' tá»« gradients<br>â†’ First-order method vá»›i second-order algorithm!"
            }
          ]
        }
      ]
    },
    {
      "id": 7,
      "type": "algorithm",
      "title": "AdaGrad Algorithm: 3 bÆ°á»›c Ä‘Æ¡n giáº£n",
      "subtitle": "Implementation steps",
      "badges": [
        { "text": "Algorithm Steps", "color": "green" },
        { "text": "Section 2.4", "color": "blue" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "3 bÆ°á»›c chÃ­nh",
          "cards": [
            {
              "icon": "1ï¸âƒ£",
              "iconColor": "blue",
              "title": "Compute Gradient",
              "content": "g<sub>t</sub> = âˆ‚<sub>w</sub> L(w<sub>t</sub>)<br><br>TÃ­nh Ä‘áº¡o hÃ m loss theo parameters<br>Vector cÃ³ cÃ¹ng dimension vá»›i w"
            },
            {
              "icon": "2ï¸âƒ£",
              "iconColor": "orange",
              "title": "Accumulate Squared Gradients",
              "content": "s<sub>t</sub> = s<sub>t-1</sub> + g<sub>t</sub> âŠ™ g<sub>t</sub><br><br><strong style='color:#f44336;'>Element-wise square!</strong><br>s<sub>t</sub>[i] = s<sub>t-1</sub>[i] + g<sub>t</sub>[i]Â²<br><br>Má»—i coordinate 'nhá»›' lá»‹ch sá»­ gradient"
            },
            {
              "icon": "3ï¸âƒ£",
              "iconColor": "green",
              "title": "Update with Adaptive LR",
              "content": "w<sub>t+1</sub> = w<sub>t</sub> - <span style='color:#4CAF50;font-weight:bold;'>Î· / âˆš(s<sub>t</sub> + Îµ)</span> âŠ™ g<sub>t</sub><br><br>Má»—i parameter cÃ³ LR riÃªng:<br>Î·<sub>eff</sub>[i] = Î· / âˆš(s<sub>t</sub>[i] + Îµ)<br><br><strong>s lá»›n â†’ Î· nhá» | s nhá» â†’ Î· lá»›n</strong>"
            }
          ]
        },
        {
          "title": "Hyperparameters",
          "cards": [
            {
              "icon": "âš™ï¸",
              "iconColor": "purple",
              "title": "CÃ i Ä‘áº·t Ä‘iá»ƒn hÃ¬nh",
              "content": "<strong>Î·:</strong> 0.01 (robust hÆ¡n SGD)<br><strong>Îµ:</strong> 10â»â¸ (trÃ¡nh chia 0)<br><strong>sâ‚€:</strong> vector 0<br><br><span style='color:#2196F3;'>Element-wise operations â†’ O(d) memory!</span>"
            }
          ]
        }
      ]
    },
    {
      "id": 8,
      "type": "algorithm",
      "title": "Case 1: Axis-Aligned Function - TÃ­nh toÃ¡n tá»«ng bÆ°á»›c",
      "subtitle": "f(x) = 0.1xâ‚Â² + 2xâ‚‚Â² - AdaGrad hoáº¡t Ä‘á»™ng hoÃ n háº£o!",
      "badges": [
        { "text": "Manual Calculation", "color": "blue" },
        { "text": "Step-by-Step", "color": "green" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Setup & Step 1",
          "cards": [
            {
              "icon": "ğŸ“‹",
              "iconColor": "blue",
              "title": "Setup",
              "content": "<strong>Function:</strong> f(x) = 0.1xâ‚Â² + 2xâ‚‚Â²<br><strong>Gradient:</strong> âˆ‡f = [0.2xâ‚, 4xâ‚‚]<br><strong>Hessian:</strong> H = diag(0.2, 4) - DIAGONAL!<br><strong>Initial:</strong> xâ½â°â¾ = (5, 3), Î· = 0.4, Îµ = 10â»â¸, sâ½â°â¾ = (0, 0)"
            },
            {
              "icon": "1ï¸âƒ£",
              "iconColor": "green",
              "title": "Step 1: Tá»« (5, 3)",
              "content": "<code style='display:block; padding:0.8rem; background:rgba(255,255,255,0.05); margin:0.5rem 0;'>gâ‚ = [0.2Ã—5, 4Ã—3] = [1.0, 12.0]<br>sâ‚ = [0, 0] + [1Â², 12Â²] = [1.0, 144.0]<br>Î·â‚‘ff = [0.4/âˆš1, 0.4/âˆš144] = [0.4, 0.0333]<br>xâ½Â¹â¾ = [5, 3] - [0.4Ã—1, 0.0333Ã—12] = [4.6, 2.6]</code><br><span style='color:#FFC107;'>ğŸ’¡ xâ‚‚ cÃ³ gradient 12Ã— lá»›n hÆ¡n â†’ sâ‚‚ tÄƒng nhanh â†’ Î· tá»± Ä‘á»™ng nhá» hÆ¡n!</span>"
            }
          ]
        },
        {
          "title": "Step 2 & algorithm",
          "cards": [
            {
              "icon": "2ï¸âƒ£",
              "iconColor": "orange",
              "title": "Step 2: Tá»« (4.6, 2.6)",
              "content": "<code style='display:block; padding:0.8rem; background:rgba(255,255,255,0.05); margin:0.5rem 0;'>gâ‚‚ = [0.92, 10.4]<br>sâ‚‚ = [1.0, 144] + [0.85, 108.2] = [1.85, 252.2]<br>Î·â‚‘ff = [0.294, 0.025]<br>xâ½Â²â¾ = [4.33, 2.34], f â‰ˆ 12.8</code><br><span style='color:#FFC107;'>ğŸ’¡ Cáº£ 2 coordinates tiáº¿n vá» 0 cÃ¢n báº±ng</span>"
            },
            {
              "icon": "âœ¨",
              "iconColor": "purple",
              "title": "Táº¡i sao hoáº¡t Ä‘á»™ng hoÃ n háº£o?",
              "content": "H diagonal â†’ variables Ä‘á»™c láº­p<br>AdaGrad's diagonal scaling = perfect match!<br>Má»—i coordinate tá»± Ä‘iá»u chá»‰nh theo curvature riÃªng"
            }
          ]
        }
      ]
    },
    {
      "id": 9,
      "type": "algorithm",
      "title": "Case 2: Rotated Function - AdaGrad gáº·p khÃ³ khÄƒn",
      "subtitle": "f(x) = 0.1(xâ‚+xâ‚‚)Â² + 2(xâ‚-xâ‚‚)Â² - Variables bá»‹ couple!",
      "badges": [
        { "text": "Coupled Variables", "color": "orange" },
        { "text": "Suboptimal", "color": "red" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Setup & Step 1",
          "cards": [
            {
              "icon": "ğŸ“‹",
              "iconColor": "orange",
              "title": "Setup",
              "content": "<strong>Function:</strong> f(x) = 0.1(xâ‚+xâ‚‚)Â² + 2(xâ‚-xâ‚‚)Â²<br><strong>Expanded:</strong> = 2.1xâ‚Â² - 3.8xâ‚xâ‚‚ + 2.1xâ‚‚Â²<br><strong>Gradient:</strong> âˆ‡f = [4.2xâ‚-3.8xâ‚‚, -3.8xâ‚+4.2xâ‚‚]<br><strong>Hessian:</strong> H = [[4.2, -3.8], [-3.8, 4.2]] - OFF-DIAGONAL!<br><span style='color:#FFC107;'>âš ï¸ CÃ¹ng eigenvalues vá»›i Function 1, nhÆ°ng rotate 45Â°</span>"
            },
            {
              "icon": "1ï¸âƒ£",
              "iconColor": "red",
              "title": "Step 1: Tá»« (5, 3)",
              "content": "<code style='display:block; padding:0.8rem; background:rgba(255,255,255,0.05); margin:0.5rem 0;'>gâ‚ = [21-11.4, -19+12.6] = [9.6, -6.4]<br>sâ‚ = [92.16, 40.96]<br>Î·â‚‘ff = [0.0417, 0.0625]<br>xâ½Â¹â¾ = [4.6, 3.4], f â‰ˆ 9.3</code><br><span style='color:#f44336;'>ğŸ’¡ xâ‚‚ tÄƒng lÃªn! KhÃ´ng tiáº¿n tháº³ng vá» (0,0)</span>"
            }
          ]
        },
        {
          "title": "Step 2 & algorithm",
          "cards": [
            {
              "icon": "2ï¸âƒ£",
              "iconColor": "orange",
              "title": "Step 2: Tá»« (4.6, 3.4)",
              "content": "<code style='display:block; padding:0.8rem; background:rgba(255,255,255,0.05); margin:0.5rem 0;'>gâ‚‚ = [6.4, -3.2]<br>sâ‚‚ = [133.12, 51.2]<br>xâ½Â²â¾ = [4.38, 3.58], f â‰ˆ 6.7</code><br><span style='color:#f44336;'>ğŸ’¡ Zigzag pattern - khÃ´ng hiá»‡u quáº£ nhÆ° Function 1</span>"
            },
            {
              "icon": "âŒ",
              "iconColor": "red",
              "title": "Táº¡i sao kÃ©m hiá»‡u quáº£?",
              "content": "Optimal direction: diagonal 45Â°<br>AdaGrad chá»‰ scale xâ‚, xâ‚‚ Ä‘á»™c láº­p<br>Cross-term -3.8xâ‚xâ‚‚ táº¡o coupling<br><br><strong>Diagonal optimizer â‰  non-diagonal problem!</strong>"
            }
          ]
        }
      ]
    },
    {
      "id": 10,
      "type": "algorithm",
      "title": "Táº¡i sao rotation láº¡i quan trá»ng?",
      "subtitle": "Geometric explanation of AdaGrad's limitation",
      "badges": [
        { "text": "Geometry", "color": "purple" },
        { "text": "Key algorithm", "color": "blue" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "CÃ¹ng Ä‘á»™ khÃ³, khÃ¡c coordinate system",
          "cards": [
            {
              "icon": "ğŸ“",
              "iconColor": "green",
              "title": "Eigenvalues giá»‘ng nhau",
              "content": "<strong>Function 1:</strong> H = diag(0.2, 4)<br>Eigenvectors: [1,0] vÃ  [0,1] (trá»¥c tá»a Ä‘á»™)<br><br><strong>Function 2:</strong> H = [[4.2,-3.8],[-3.8,4.2]]<br>Eigenvectors: [1,1]/âˆš2 vÃ  [1,-1]/âˆš2 (Ä‘Æ°á»ng chÃ©o 45Â°)<br><br><span style='color:#4CAF50;'>CÃ¹ng eigenvalues (0.4, 8) - cÃ¹ng Ä‘á»™ khÃ³!</span>"
            },
            {
              "icon": "ğŸ¯",
              "iconColor": "blue",
              "title": "AdaGrad = Diagonal optimizer",
              "content": "x<sub>t+1</sub> = x<sub>t</sub> - D<sub>t</sub><sup>-1/2</sup> g<sub>t</sub><br>D<sub>t</sub> = diag(1/âˆšsâ‚, 1/âˆšsâ‚‚)<br><br><strong>Chá»‰ scale tá»«ng coordinate Ä‘á»™c láº­p!</strong><br><br>Function 1: Perfect match âœ“<br>Function 2: Misalignment âœ—"
            }
          ]
        },
        {
          "title": "BÃ i há»c quan trá»ng",
          "cards": [
            {
              "icon": "ğŸ’¡",
              "iconColor": "purple",
              "title": "Khi nÃ o AdaGrad hoáº¡t Ä‘á»™ng tá»‘t?",
              "content": "<strong>AdaGrad works best when:</strong><br>âœ“ Hessian diagonal (axis-aligned)<br>âœ“ Features Ä‘á»™c láº­p<br>âœ“ Variables khÃ´ng coupled<br><br><strong>Struggles when:</strong><br>âœ— Hessian cÃ³ off-diagonal terms<br>âœ— Features coupled/correlated<br>âœ— Rotated coordinate system<br><br><span style='color:#2196F3;'>Solutions: Momentum (Adam), full Newton, or rotate coordinates!</span>"
            }
          ]
        }
      ]
    },
    {
      "id": 11,
      "type": "algorithm",
      "title": "Experimental algorithms: Robustness Test",
      "subtitle": "AdaGrad vá»›i conservative vs extreme learning rates",
      "badges": [
        { "text": "Î·=0.2", "color": "blue" },
        { "text": "Î·=3.0", "color": "red" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Scenario 1: Conservative Î·=0.2",
          "cards": [
            {
              "icon": "âš ï¸",
              "iconColor": "orange",
              "title": "Váº¥n Ä‘á»: Premature Stopping",
              "content": "s<sub>t</sub> tÄƒng Ä‘Æ¡n Ä‘iá»‡u â†’ Î·<sub>eff</sub> = Î·/âˆšs<sub>t</sub> â†’ 0<br><br>Algorithm 'Ä‘Ã³ng bÄƒng' trÆ°á»›c khi Ä‘áº¡t optimum<br><br><strong>BÃ i há»c:</strong> LR quÃ¡ nhá» + accumulation â†’ vanishing LR"
            },
            {
              "icon": "ğŸ“Š",
              "iconColor": "blue",
              "title": "Observation",
              "content": "Trajectory báº¯t Ä‘áº§u tá»‘t nhÆ°ng dá»«ng giá»¯a chá»«ng<br>Cannot reach (0,0) trong 20 steps"
            }
          ]
        },
        {
          "title": "Scenario 2: Extreme Î·=3.0",
          "cards": [
            {
              "icon": "âœ…",
              "iconColor": "green",
              "title": "Káº¿t quáº£: Tá»± Ä‘á»™ng á»•n Ä‘á»‹nh!",
              "content": "DÃ¹ Î· cá»±c lá»›n, váº«n converge thÃ nh cÃ´ng!<br><br><strong>CÆ¡ cháº¿:</strong><br>Direction dá»‘c (xâ‚‚) â†’ gradient lá»›n<br>â†’ sâ‚‚ explode â†’ Î·<sub>eff</sub> tá»± Ä‘á»™ng nhá»<br><br><span style='color:#4CAF50;'>Automatic brake mechanism!</span>"
            },
            {
              "icon": "ğŸ’¡",
              "iconColor": "purple",
              "title": "Robustness",
              "content": "Demonstrates AdaGrad's self-regulation<br>Less sensitive to initial LR choice<br>Steep directions get automatic damping"
            }
          ]
        }
      ]
    },
    {
      "id": 12,
      "type": "image",
      "title": "Visualize: Conservative LR (Î·=0.2)",
      "subtitle": "Smooth nhÆ°ng dá»«ng sá»›m",
      "badges": [
        { "text": "Experiment", "color": "blue" }
      ],
      "image": "../image/Adagrad-Trajectory-lr=0.2.png",
      "imageStyle": "max-height: 480px; object-fit: contain;",
      "algorithm": "Trajectory moves toward optimum but freezes midway - vanishing LR problem!"
    },
    {
      "id": 13,
      "type": "image",
      "title": "Visualize: Extreme LR (Î·=3.0)",
      "subtitle": "Aggressive nhÆ°ng tá»± Ä‘iá»u chá»‰nh",
      "badges": [
        { "text": "Robustness", "color": "green" }
      ],
      "image": "../image/Adagrad-Trajectory-r=3.0.png",
      "imageStyle": "max-height: 480px; object-fit: contain;",
      "algorithm": "Despite huge LR, converges successfully - automatic brake in steep directions!"
    },
    {
      "id": 14,
      "type": "algorithm",
      "title": "Fashion-MNIST: Real-world Test",
      "subtitle": "LeNet with AdaGrad - Does it work on neural networks?",
      "badges": [
        { "text": "CNN", "color": "blue" },
        { "text": "10 epochs", "color": "green" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Setup & Results",
          "cards": [
            {
              "icon": "ğŸ§ ",
              "iconColor": "blue",
              "title": "Model: Improved LeNet",
              "content": "Conv â†’ ReLU â†’ MaxPool â†’ Conv â†’ ReLU â†’ MaxPool â†’ FC(120) â†’ FC(84) â†’ FC(10)<br><br>Replaced Sigmoid with ReLU to avoid vanishing gradients"
            },
            {
              "icon": "ğŸ“ˆ",
              "iconColor": "green",
              "title": "Training Performance",
              "content": "Optimizer: AdaGrad(lr=0.01)<br>Batch size: 256<br><br>âœ“ Loss giáº£m Ä‘á»u<br>âœ“ Accuracy tÄƒng á»•n Ä‘á»‹nh<br>âœ“ Demonstrates effectiveness on CNNs"
            }
          ]
        },
        {
          "title": "Observations",
          "cards": [
            {
              "icon": "âš ï¸",
              "iconColor": "orange",
              "title": "Note vá» Deep Networks",
              "content": "10 epochs: AdaGrad works fine<br><br>Longer training (50+ epochs):<br>â†’ May suffer from vanishing LR<br>â†’ Consider RMSProp/Adam instead"
            },
            {
              "icon": "ğŸ’¡",
              "iconColor": "purple",
              "title": "When to use AdaGrad",
              "content": "âœ“ Short training (few epochs)<br>âœ“ Sparse features dominant<br>âœ“ Convex or near-convex problems<br><br>âœ— Very deep networks<br>âœ— Long training sessions"
            }
          ]
        }
      ]
    },
    {
      "id": 15,
      "type": "image",
      "title": "Fashion-MNIST: Training Loss",
      "subtitle": "Steady decrease over 10 epochs",
      "badges": [
        { "text": "Results", "color": "green" }
      ],
      "image": "../image/Training-Loss.png",
      "imageStyle": "max-height: 450px; object-fit: contain;"
    },
    {
      "id": 16,
      "type": "image",
      "title": "Fashion-MNIST: Test Accuracy",
      "subtitle": "Convergence pattern",
      "badges": [
        { "text": "Performance", "color": "blue" }
      ],
      "image": "../image/Test-Accuracy.png",
      "imageStyle": "max-height: 450px; object-fit: contain;"
    },
    {
      "id": 17,
      "type": "algorithm",
      "title": "AdaGrad Advantages - Khi nÃ o nÃªn dÃ¹ng?",
      "subtitle": "Strengths vÃ  best use cases",
      "badges": [
        { "text": "Sparse Data", "color": "green" },
        { "text": "NLP", "color": "blue" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Core Strengths",
          "cards": [
            {
              "icon": "ğŸ¯",
              "iconColor": "green",
              "title": "1. Automatic Per-Parameter Adaptation",
              "content": "<strong>KhÃ´ng cáº§n tune LR cho tá»«ng parameter!</strong><br><br>Frequent features â†’ tá»± Ä‘á»™ng giáº£m LR<br>Rare features â†’ tá»± Ä‘á»™ng tÄƒng LR<br><br>Tiáº¿t kiá»‡m effort tuning hyperparameters"
            },
            {
              "icon": "ğŸ’ª",
              "iconColor": "blue",
              "title": "2. Robust to Large Initial LR",
              "content": "<strong>Automatic brake mechanism</strong><br><br>Steep directions self-regulate<br>s explodes â†’ Î·<sub>eff</sub> shrinks<br><br>Less sensitive to Î·<sub>0</sub> choice"
            },
            {
              "icon": "âš¡",
              "iconColor": "purple",
              "title": "3. Efficient for Sparse Features",
              "content": "<strong>O(d) memory overhead</strong><br><br>Only active features accumulate<br>Simple element-wise operations<br>No matrix operations needed"
            }
          ]
        },
        {
          "title": "Best Applications",
          "cards": [
            {
              "icon": "ğŸ“",
              "iconColor": "blue",
              "title": "Natural Language Processing",
              "content": "âœ“ Bag-of-words, TF-IDF<br>âœ“ Word embeddings (rare words!)<br>âœ“ Text classification<br>âœ“ Language modeling"
            },
            {
              "icon": "ğŸ¬",
              "iconColor": "green",
              "title": "Recommender Systems",
              "content": "âœ“ User/item embeddings<br>âœ“ Many users have few interactions<br>âœ“ Sparse interaction matrices<br>âœ“ Cold-start items"
            },
            {
              "icon": "ğŸ“±",
              "iconColor": "orange",
              "title": "Computational Advertising",
              "content": "âœ“ CTR prediction<br>âœ“ Sparse features: user IDs, ads<br>âœ“ Rare ads need effective learning<br>âœ“ Feature engineering heavy"
            }
          ]
        }
      ]
    },
    {
      "id": 18,
      "type": "warning",
      "title": "AdaGrad Limitations - Khi nÃ o trÃ¡nh dÃ¹ng?",
      "subtitle": "Critical weaknesses vÃ  failure modes",
      "badges": [
        { "text": "Vanishing LR", "color": "red" },
        { "text": "Deep Learning", "color": "orange" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Main Weaknesses",
          "cards": [
            {
              "icon": "ğŸ“‰",
              "iconColor": "red",
              "title": "1. Monotonic LR Decay (FATAL!)",
              "content": "<strong>s<sub>t</sub> = s<sub>t-1</sub> + g<sub>t</sub>Â² luÃ´n tÄƒng</strong><br><br>Î·<sub>eff</sub> = Î·/âˆšs<sub>t</sub> â†’ 0 inevitably<br><br><span style='color:#f44336;'>Can stop learning before convergence!</span><br>Especially problematic in deep NNs"
            },
            {
              "icon": "ğŸ”„",
              "iconColor": "orange",
              "title": "2. Poor for Non-Convex Landscapes",
              "content": "<strong>Deep NNs: plateaus, saddle points</strong><br><br>Need ability to 'forget' old gradients<br>AdaGrad accumulates forever<br><br>Cannot adapt to changing landscape"
            },
            {
              "icon": "ğŸ“",
              "iconColor": "purple",
              "title": "3. Diagonal Limitation",
              "content": "<strong>Only axis-aligned problems</strong><br><br>Struggles with coupled features<br>Cannot capture off-diagonal Hessian<br><br>Rotated problems â†’ suboptimal"
            }
          ]
        },
        {
          "title": "When to Avoid",
          "cards": [
            {
              "icon": "ğŸ§ ",
              "iconColor": "red",
              "title": "Deep Neural Networks",
              "content": "âŒ Long training (50+ epochs)<br>âŒ Very deep architectures<br>âŒ Need sustained learning<br><br><strong>Use instead:</strong> RMSProp, Adam"
            },
            {
              "icon": "ğŸ”€",
              "iconColor": "orange",
              "title": "Non-Stationary Problems",
              "content": "âŒ Data distribution shifts<br>âŒ Online learning with drift<br>âŒ Transfer learning scenarios<br><br>Need adaptive but not monotonic"
            },
            {
              "icon": "ğŸ²",
              "iconColor": "purple",
              "title": "Highly Non-Convex",
              "content": "âŒ Reinforcement learning<br>âŒ GANs (generative models)<br>âŒ Complex multi-modal landscapes<br><br>Need exploration capability"
            }
          ]
        }
      ]
    },
    {
      "id": 19,
      "type": "algorithm",
      "title": "Evolution: Tá»« AdaGrad Ä‘áº¿n Adam",
      "subtitle": "Fixing vanishing LR problem",
      "badges": [
        { "text": "RMSProp", "color": "blue" },
        { "text": "Adam", "color": "green" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "AdaGrad â†’ RMSProp",
          "cards": [
            {
              "icon": "ğŸ”µ",
              "iconColor": "blue",
              "title": "AdaGrad (2011): Innovation",
              "content": "<strong>s<sub>t</sub> = s<sub>t-1</sub> + g<sub>t</sub>Â²</strong><br><br>âœ“ Per-parameter adaptive LR<br>âœ“ Great for sparse features<br>âœ— Monotonic accumulation â†’ vanishing LR"
            },
            {
              "icon": "ğŸŸ¢",
              "iconColor": "green",
              "title": "RMSProp (2012): Fix vanishing LR",
              "content": "<strong>s<sub>t</sub> = Î³s<sub>t-1</sub> + (1-Î³)g<sub>t</sub>Â²</strong><br><br>Exponential moving average (Î³â‰ˆ0.9)<br>'Forgets' old gradients<br>s<sub>t</sub> khÃ´ng grow unbounded<br><br><span style='color:#4CAF50;'>â†’ LR stabilizes!</span>"
            }
          ]
        },
        {
          "title": "Modern Standard",
          "cards": [
            {
              "icon": "ğŸŸ£",
              "iconColor": "purple",
              "title": "Adam (2015): Best of both worlds",
              "content": "<strong>Momentum + RMSProp</strong><br><br>m<sub>t</sub>: 1st moment (momentum)<br>v<sub>t</sub>: 2nd moment (RMSProp)<br><br>Combines direction smoothing + adaptive scaling<br><br><span style='color:#2196F3;'>â†’ Current standard in deep learning!</span>"
            }
          ]
        }
      ]
    },
    {
      "id": 20,
      "type": "algorithm",
      "title": "Exercise: Fixing AdaGrad â†’ RMSProp",
      "subtitle": "Implementing exponential moving average",
      "badges": [
        { "text": "Hands-on", "color": "blue" },
        { "text": "Comparison", "color": "green" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Problem & Solution",
          "cards": [
            {
              "icon": "âš ï¸",
              "iconColor": "red",
              "title": "Problem identified",
              "content": "<strong>AdaGrad:</strong> s<sub>t</sub> accumulates indefinitely<br>â†’ Learning rate vanishes too early<br>â†’ Model stops learning before optimal<br><br>Visible in Fashion-MNIST: loss plateaus early"
            },
            {
              "icon": "ğŸ’¡",
              "iconColor": "green",
              "title": "Proposed fix: Exponential Moving Average",
              "content": "<strong>Modified accumulation:</strong><br>s<sub>t</sub> = Î³s<sub>t-1</sub> + (1-Î³)g<sub>t</sub>Â²<br><br>Î³ = 0.9 typical (90% old, 10% new)<br><br>This is exactly <strong>RMSProp</strong>!<br>Allows 'forgetting' old gradients"
            }
          ]
        },
        {
          "title": "Experimental Results",
          "cards": [
            {
              "icon": "ğŸ“Š",
              "iconColor": "blue",
              "title": "Comparison on Fashion-MNIST",
              "content": "<strong>AdaGrad:</strong> Training loss plateaus ~epoch 5<br><strong>RMSProp:</strong> Continues learning longer<br><br>Test accuracy improves significantly<br><br><span style='color:#4CAF50;'>EMA prevents LR vanishing - critical for deep NNs!</span><br><br>Simple 1-line change â†’ huge impact"
            }
          ]
        }
      ]
    },
    {
      "id": 21,
      "type": "image",
      "title": "Comparison: AdaGrad vs RMSProp Training Loss",
      "subtitle": "Impact of exponential moving average",
      "badges": [
        { "text": "Experiment", "color": "blue" }
      ],
      "image": "../image/Training-Loss-Comparison.png",
      "imageStyle": "max-height: 450px; object-fit: contain;",
      "algorithm": "RMSProp continues learning while AdaGrad plateaus - demonstrates vanishing LR problem"
    },
    {
      "id": 22,
      "type": "image",
      "title": "Comparison: AdaGrad vs RMSProp Test Accuracy",
      "subtitle": "Better long-term performance",
      "badges": [
        { "text": "Results", "color": "green" }
      ],
      "image": "../image/Test-Accuracy-Comparison.png",
      "imageStyle": "max-height: 450px; object-fit: contain;",
      "algorithm": "Simple modification (EMA) makes huge difference - foundation for Adam optimizer"
    },
    {
      "id": 23,
      "type": "algorithm",
      "title": "Summary: Key Takeaways",
      "subtitle": "Nhá»¯ng Ä‘iá»u quan trá»ng nháº¥t vá» AdaGrad",
      "badges": [
        { "text": "Core Concepts", "color": "blue" },
        { "text": "Practical Wisdom", "color": "green" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Core Understanding",
          "cards": [
            {
              "icon": "ğŸ¯",
              "iconColor": "blue",
              "title": "Innovation: Per-parameter adaptive LR",
              "content": "<strong>s<sub>t</sub> = s<sub>t-1</sub> + g<sub>t</sub>Â²</strong><br>Î·<sub>eff</sub> = Î·/âˆš(s<sub>t</sub> + Îµ)<br><br>Squared gradients as Hessian proxy<br>Diagonal preconditioning interpretation"
            },
            {
              "icon": "âœ…",
              "iconColor": "green",
              "title": "When it shines",
              "content": "<strong>Best for:</strong><br>âœ“ Sparse features (NLP, RecSys)<br>âœ“ Axis-aligned problems<br>âœ“ Convex or near-convex<br>âœ“ Short training sessions"
            },
            {
              "icon": "âŒ",
              "iconColor": "red",
              "title": "When to avoid",
              "content": "<strong>Struggles with:</strong><br>âœ— Deep neural networks<br>âœ— Long training (50+ epochs)<br>âœ— Non-stationary landscapes<br>âœ— Coupled/rotated features"
            }
          ]
        },
        {
          "title": "Practical Guidance",
          "cards": [
            {
              "icon": "ğŸ”§",
              "iconColor": "purple",
              "title": "Implementation",
              "content": "Simple: O(d) memory<br>Element-wise operations only<br><br><strong>PyTorch:</strong><br>torch.optim.Adagrad(params, lr=0.01)<br><br>More robust to LR choice than SGD"
            },
            {
              "icon": "ğŸš€",
              "iconColor": "orange",
              "title": "Modern alternatives",
              "content": "<strong>RMSProp:</strong> Fix vanishing LR (EMA)<br><strong>Adam:</strong> Add momentum<br><strong>AdamW:</strong> Proper weight decay<br><br>For deep learning: use Adam as default"
            },
            {
              "icon": "ğŸ“š",
              "iconColor": "green",
              "title": "Historical impact",
              "content": "Pioneered adaptive methods (2011)<br>Inspired entire family of optimizers<br>Still effective for specific domains<br><br>Foundation for understanding modern optimizers"
            }
          ]
        }
      ]
    },
    {
      "id": 24,
      "type": "algorithm",
      "title": "NhÃ³m há»c Ä‘Æ°á»£c gÃ¬ tá»« AdaGrad?",
      "subtitle": "Beyond the algorithm - deeper algorithms",
      "badges": [
        { "text": "Reflection", "color": "purple" },
        { "text": "Learning", "color": "blue" }
      ],
      "layout": "two-column",
      "columns": [
        {
          "title": "Hiá»ƒu sÃ¢u, khÃ´ng há»c váº¹t",
          "cards": [
            {
              "icon": "ğŸ§ ",
              "iconColor": "blue",
              "title": "1. Hiá»ƒu táº¡i sao, khÃ´ng chá»‰ biáº¿t lÃ m sao",
              "content": "<strong>KhÃ´ng chá»‰ Ã¡p dá»¥ng cÃ´ng thá»©c</strong><br><br>âœ“ Táº¡i sao squared gradients?<br>âœ“ Táº¡i sao chia âˆšs thay vÃ¬ s?<br>âœ“ Preconditioning cÃ³ nghÄ©a gÃ¬?<br>âœ“ Khi nÃ o diagonal approximation Ä‘á»§ tá»‘t?<br><br><em>Understanding > memorizing</em>"
            },
            {
              "icon": "ğŸ”¢",
              "iconColor": "green",
              "title": "2. TÃ­nh toÃ¡n tá»«ng bÆ°á»›c = verification",
              "content": "<strong>Manual calculations build intuition</strong><br><br>âœ“ Axis-aligned: perfect convergence<br>âœ“ Rotated: struggles visibly<br>âœ“ Conservative LR: premature stopping<br>âœ“ Extreme LR: automatic stabilization<br><br><em>Numbers tell the story</em>"
            }
          ]
        },
        {
          "title": "Limitations & Trade-offs",
          "cards": [
            {
              "icon": "ğŸ’¡",
              "iconColor": "purple",
              "title": "3. Limitations = opportunities",
              "content": "<strong>Vanishing LR problem</strong><br>â†’ RMSProp (exponential MA)<br>â†’ Adam (add momentum)<br>â†’ Entire family of optimizers<br><br><em>Every weakness teaches something</em>"
            },
            {
              "icon": "âš–ï¸",
              "iconColor": "orange",
              "title": "4. Trade-offs everywhere",
              "content": "<strong>No perfect optimizer</strong><br><br>AdaGrad: Great for sparse, fails for deep<br>Adam: General-purpose, not always best<br>SGD+momentum: Simple, requires tuning<br><br><em>Tool selection = understanding context</em>"
            }
          ]
        }
      ]
    },
    {
      "id": 25,
      "type": "thank-you",
      "title": "Thank You!",
      "subtitle": "Questions & Discussion",
      "badges": [
        { "text": "Q&A", "color": "blue" },
        { "text": "AdaGrad", "color": "green" }
      ],
      "closing": {
        "message": "Cáº£m Æ¡n tháº§y vÃ  cÃ¡c báº¡n Ä‘Ã£ láº¯ng nghe!",
        "reflection": "AdaGrad khÃ´ng chá»‰ lÃ  cÃ´ng thá»©c - Ä‘Ã³ lÃ  cÃ¡ch tÆ° duy vá» optimization"
      }
    }
  ]
};
